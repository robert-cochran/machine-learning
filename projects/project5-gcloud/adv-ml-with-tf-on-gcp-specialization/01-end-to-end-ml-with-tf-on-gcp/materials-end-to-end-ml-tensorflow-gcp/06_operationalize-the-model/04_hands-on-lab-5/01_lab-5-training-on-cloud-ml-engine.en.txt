You are now ready to operationalize the training of your TensorFlow model. In this lab, you will package up your model code and submit it to Cloud ML Engine for distributed training. You will train the model on data produced by the Cloud Dataflow job. In addition to distributed training, you will also see how to do hyperparameter tuning using Cloud ML Engine. By making all the configurable parameters as command line parameters, you are getting yourself set to be able to do hyperparameter tuning. All that you then need to do is to write a configuration file with the parameters that you want to tune. But remember that we want to allow the user to be able to change the batch size. But typically, users will want to specify the number of training examples that they want to train on. But the training steps depends on the batch size. So, you want to compute the number of training steps from the number of training examples and the batch size, and you want to do this computation in your code. We want to make our hyper-parameters command-line parameters, so that we can quickly try different training configurations. By doing this, by making them command-line parameters, it happens to also facilitate running hyper-parameter tuning jobs. Once you submit the training job, monitor the progress of training using TensorBoard. Verify that your loss metric converges and that your evaluation metric doesn't start going back up. Make sure that your layers don't die as demonstrated by the fraction of zero values. With that, please go ahead and do lab number 5.