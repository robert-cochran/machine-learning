Now that we have the full dataset, the next step in operationalizing the model, is to train the model on the full dataset. Previously, we trained the model in data lab, and we could do that because it was only about 10,000 examples, now we have several million. It's time to roll out the powerful tools, to do training on the cloud, on multiple machines. We will do training using Cloud ML Engine. In order to submit code to ML Engine, we'll need a TensorFlow model to be packaged up, as a Python package. A best practice is to split our code up across at least two files. By convention, we will call the file with the main function as task.py, that file will contain the code to parse command-line arguments. Any part of our code that's configurable, or whose value is known only at runtime, we will make it a command-line argument. So in this case, trained data paths and trained steps are command-line arguments. The other file, the one with all of our TensorFlow code, including the train and evaluate loop, is by convention, called model.py. Task.py calls the code and model.py and sends in the command-line arguments. The code itself is identical to the code in the third lab. Only that all the hard-coded values are replaced, by pulling out the appropriate value from the command-line arguments. So for example, instead of just hard coding a batch size, we'll be using args['train_batch_size']. Similarly, the number of training steps will come from a command line argument. The model.py contains all the code that was in our Jypiter notebook, when we were developing the model. It will have to have the training and evaluation input functions, the definition of the feature columns, any feature engineering such as feature crosses, embeddings, marketization et cetera, that we do. Remember also to include a serving input function, so that you can easily deploy the model and invoke it as a web service. We'll review the serving input function in the next section. Cloud MLA involves packaging up TensorFlow models, this is not very different from how you need to create a web archive, if you want to deploy a Java web application. Similarly, you put your TensorFlow code in a very specific packaging structure and you deploy it to the cloud. The nice thing is, that this is a standard way to create Python modules. If you're not familiar with Python packaging, please search for minimal python packaging and read the docs for that. Once you've taken your model code and put it into a Python package, I strongly recommend, that you run your code as a Python module, against your tiny 10,000 sample dataset, just to make sure that it all works as intended. So here, that's what I'm doing, I'm calling Python with minus m, passing in the module from a package, that I want to run, and I'm trying out all of these command-line parameters that I have now added to task.py. So in the next lesson, we will look at training and deploying to ML Engine.