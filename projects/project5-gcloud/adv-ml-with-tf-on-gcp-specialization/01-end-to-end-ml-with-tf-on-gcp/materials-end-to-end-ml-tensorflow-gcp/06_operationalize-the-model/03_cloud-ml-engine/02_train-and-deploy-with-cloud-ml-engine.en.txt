In this lesson, we will quickly recap how to train, serve and monitor production machine learning models using Cloud ML Engine. Now this diagram, you've already seen before. You've seen the different abstraction layers are distributed training, but how do you actually run distributed TensorFlow at scale in production? For that, use Cloud Machine Learning Engine. Cloud ML Engine is an execution environment for TensorFlow code on a variety of cloud hardware. Whether it's CPUs, GPUs or even TPUs. Use the gcloud command to submit the training job. You can try it locally using "gcloud ml- engine local train", but primarily what you want to do is "gcloud ml-engine jobs submit training". This will take your Python code, your Python module, and submit it. This is specified using "-- module-name" and then it will call the main method on your module. The Python modules should be staged in the staging bucket, the logs will show up in the job-dir. I recommend that you also use a job-dir as the output directory as well. The scale-tier controls the execution environment. Basic_GPU would run on a single GPU, for example. The list of scale-tiers keeps expanding, so please look at the documentation for what scale-tiers are available and what the machine configuration is for each tier. The GCP web console has a great UI for monitoring your jobs. You can see exactly how they were invoked, checkout their logs, and see how much CPU and memory they're consuming. You can also view CPU and memory utilization for a training job with StackDriver Monitoring. While inspecting log entries may help you debug technical issues like an exception, it's really not the right tool to investigate machine learning performance. TensorBoard let's you do that. To use TensorBoard make sure that your job saves summary data to a Google Cloud storage location and then when you start TensorBoard, provide that directory. It can even handle multiple jobs per folder and if you're using pre-made estimators, they automatically populate summary data so you can examine and visualize them using TensorBoard.