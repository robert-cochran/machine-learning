1
00:00:00,000 --> 00:00:04,860
So we are using TensorFlow because it can scale to big data,

2
00:00:04,860 --> 00:00:07,825
capture many types of feature transformations,

3
00:00:07,825 --> 00:00:13,050
and because it has implementations of many kinds of model architectures.

4
00:00:13,050 --> 00:00:16,995
What else does a machine learning framework need to provide?

5
00:00:16,995 --> 00:00:20,070
Hyper-parameter tuning might be nice.

6
00:00:20,070 --> 00:00:22,020
When you do machine learning,

7
00:00:22,020 --> 00:00:24,905
you often pick a number of things arbitrarily.

8
00:00:24,905 --> 00:00:27,475
The number of nodes, the embedding,

9
00:00:27,475 --> 00:00:29,905
the stride size on your convolutional layer,

10
00:00:29,905 --> 00:00:31,955
the number of buckets inner embedding,

11
00:00:31,955 --> 00:00:35,050
and as your models get more and more complex,

12
00:00:35,050 --> 00:00:39,740
you start wondering whether you picked the right things when you trained the model.

13
00:00:39,740 --> 00:00:42,740
You will have to do some search in the hyper-parameter space

14
00:00:42,740 --> 00:00:46,115
to see if there are better choices that you could have made.

15
00:00:46,115 --> 00:00:49,970
So, hyper-parameter tuning might be nice.

16
00:00:50,480 --> 00:00:55,920
So far, we focused on training on the top part of this diagram.

17
00:00:55,920 --> 00:01:00,455
But besides training, you might also want

18
00:01:00,455 --> 00:01:05,180
to autoscale prediction code because at some point,

19
00:01:05,180 --> 00:01:08,800
you want to take a train the model and you want to deploy it.

20
00:01:08,800 --> 00:01:15,290
At that point, the performance characteristic that you're concerned about changes.

21
00:01:15,290 --> 00:01:19,800
Instead of thinking about how long it takes to train on your training dataset,

22
00:01:19,800 --> 00:01:23,390
you start thinking about how many queries per

23
00:01:23,390 --> 00:01:27,200
second do you need to support from your client code.

24
00:01:27,200 --> 00:01:31,070
This requires autoscaling the prediction code

25
00:01:31,070 --> 00:01:35,330
as necessary to support the users who need those predictions.

26
00:01:35,330 --> 00:01:38,600
Do you want all your client code creating

27
00:01:38,600 --> 00:01:43,710
a DNN regressor parsing in the directory name and calling predict on it?

28
00:01:43,710 --> 00:01:45,755
What if your model changes?

29
00:01:45,755 --> 00:01:47,845
What if the hyper-parameters change?

30
00:01:47,845 --> 00:01:49,830
The number of inputs changes?

31
00:01:49,830 --> 00:01:53,110
Do you really want to expose those to your users?

32
00:01:53,110 --> 00:01:56,080
What if the client is not Python?

33
00:01:56,080 --> 00:02:01,025
The answer is to wrap your model with a Rest API.

34
00:02:01,025 --> 00:02:06,035
You can then invoke the API from pretty much any programming language.

35
00:02:06,035 --> 00:02:09,620
You can put the server on the cloud and

36
00:02:09,620 --> 00:02:14,680
scale to as many queries as you need when you need it.

37
00:02:15,390 --> 00:02:21,435
But not as obvious is this question,

38
00:02:21,435 --> 00:02:24,080
who does the pre-processing?

39
00:02:24,080 --> 00:02:30,170
Who will do the input transformations on behalf of the client code?

40
00:02:30,170 --> 00:02:35,170
You can't pass in the raw input variables to the trained model because

41
00:02:35,170 --> 00:02:40,755
a trained model expects scaled transformed inputs.

42
00:02:40,755 --> 00:02:43,895
You also have to worry about model changes.

43
00:02:43,895 --> 00:02:46,410
When you do a bag of words, for example,

44
00:02:46,410 --> 00:02:49,755
with a certain word mapped to the number 32,

45
00:02:49,755 --> 00:02:54,610
the embedding might change the next model run because your input data is larger,

46
00:02:54,610 --> 00:02:57,470
and now that word gets mapped to a different number,

47
00:02:57,470 --> 00:02:59,330
say the number 56.

48
00:02:59,330 --> 00:03:02,290
Similarly and scaling the minimum,

49
00:03:02,290 --> 00:03:04,530
the maximum, the standard deviation,

50
00:03:04,530 --> 00:03:06,100
these can all change.

51
00:03:06,100 --> 00:03:08,920
So, doing the bookkeeping associated with

52
00:03:08,920 --> 00:03:14,575
pre-processing and feature crosses is painful and it's a major source of error.

53
00:03:14,575 --> 00:03:17,240
It's also near impossible to debug,

54
00:03:17,240 --> 00:03:22,540
so there are probably many models out there that have a training serving skew.

55
00:03:22,540 --> 00:03:26,260
This difference between what it was trained

56
00:03:26,260 --> 00:03:30,580
on and what it's being presented at prediction time.

57
00:03:30,580 --> 00:03:33,535
Cloud ML Engine gives you the ability to carry out

58
00:03:33,535 --> 00:03:38,265
machine learning that is repeatable, scalable, and tuned.

59
00:03:38,265 --> 00:03:40,630
Repeatable, well you need

60
00:03:40,630 --> 00:03:44,665
a machine learning framework that helps you handle training serving skew.

61
00:03:44,665 --> 00:03:47,755
The use of TensorFlow transform, for example,

62
00:03:47,755 --> 00:03:53,250
simplifies the bookkeeping in several common situations. It's scalable.

63
00:03:53,250 --> 00:03:56,500
In training, Cloud ML will help you

64
00:03:56,500 --> 00:03:59,960
distribute the pre-processing and training of your model.

65
00:03:59,960 --> 00:04:03,540
It will also help you deploy your trained model to the Cloud.

66
00:04:03,540 --> 00:04:07,195
This is important because you need high-quality execution,

67
00:04:07,195 --> 00:04:10,180
both during training and in prediction time.

68
00:04:10,180 --> 00:04:15,060
Of course, you can use Cloud ML Engine to do hyper-parameter tuning.

69
00:04:16,030 --> 00:04:18,550
The way that we will work,

70
00:04:18,550 --> 00:04:21,450
is that we will start in Cloud Datalab,

71
00:04:21,450 --> 00:04:23,435
a Jupiter notebook environment.

72
00:04:23,435 --> 00:04:26,360
We'll run sequel statements to aggregate the data in

73
00:04:26,360 --> 00:04:30,575
BigQuery and pull the data into a Pandas DataFrame.

74
00:04:30,575 --> 00:04:35,750
We can then do exploration and feature selection and pre-processing using Pandas,

75
00:04:35,750 --> 00:04:39,140
and we will write this Pandas DataFrame out to CSV.

76
00:04:39,140 --> 00:04:42,835
Then, we'll start experimenting with TensorFlow.

77
00:04:42,835 --> 00:04:48,300
Once we have a working TensorFlow model on the smaller dataset,

78
00:04:48,300 --> 00:04:52,665
we can then scale it out to Google Cloud Platform.

79
00:04:52,665 --> 00:04:55,925
We'll do this using serverless technologies.

80
00:04:55,925 --> 00:04:59,190
We'll do a pre-processing in Cloud Dataflow

81
00:04:59,190 --> 00:05:02,320
so that the pre-processing can be scaled out to many machines.

82
00:05:02,320 --> 00:05:06,990
We'll create sharded CSV files on cloud storage.

83
00:05:06,990 --> 00:05:09,530
The TensorFlow model will remain the same,

84
00:05:09,530 --> 00:05:14,405
but we'll train the model on Cloud ML Engine so that we get distributed training.

85
00:05:14,405 --> 00:05:18,565
We'll do hyper-parameter tuning and deployment also on ML Engine.

86
00:05:18,565 --> 00:05:20,500
So Dataflow and ML Engine,

87
00:05:20,500 --> 00:05:22,550
these are both serverless technologies.

88
00:05:22,550 --> 00:05:28,395
So, all that we will need is Python code to submit to the services.

89
00:05:28,395 --> 00:05:32,765
So, this is a set of labs that you will be doing.

90
00:05:32,765 --> 00:05:37,110
The first lab is to explore and visualize a dataset.

91
00:05:37,110 --> 00:05:39,205
That's what you'll do in the next module.

92
00:05:39,205 --> 00:05:41,945
But before we send you off to do the lab,

93
00:05:41,945 --> 00:05:44,175
we will talk about the problem itself,

94
00:05:44,175 --> 00:05:47,330
and review some concepts before you go off to do the lab.

95
00:05:47,330 --> 00:05:51,990
Concepts that you will need in order to accomplish the tasks in the lab.