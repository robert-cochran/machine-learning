Now, we're going to walk through the solution for the lab you just tried on creating a TensorFlow model. If you haven't already tried this lab, go back and try it in Qwiklabs. If you have already tried it, know that you have multiple attempts for each lab. So, if you see something cool in this walkthrough, feel free to go back and try it again. I recommend that you build a linear model, a deep neural network model, and a wide and deep model, or you can just build the wide and deep model. It's good practice though just start out with a simple model if you can. Plus, pre-built estimators in TensorFlow make it easy to try out different model architectures. All right, let's get started and build TensorFlow models. So, you'll notice that there's actually two files for this third lab. There is the DNN file and the WD. The DNN is the Deep Neural Network and the WD stands for Wide and Deep. Let's walk through the DNN section and then we'll do the wide and deep afterwards. So, if we go to the third one, for DNN. Earlier, we spent time creating the sample data set in the prior lab. Why did we do this? Well, we wanted to use this small sub-sample of data so we can develop a model locally. Once we're satisfied with our local model, we can then scale it out to the cloud to get real performance evaluation. For now, let's build this local model in TensorFlow. So as we've been doing before, we need to specify the bucket in the project and the region of interest. So, the bucket in the project are specific to you, in this case, minus crawls sandbox, for the bucket and the project, you can set them again as environmental variables, and we will keep proceeding on. Oh, you should have your CSV files from the prior lab, or evaluation CSV and our chain CSV. We will use these to train our models and evaluate them. All right, so let's use the TensorFlow estimator API. So, we're going to import the libraries of interest that we need to complete this lab, including Shtil, numpy in TensorFlow. So, there's really two key components to this entire lab right, there's number one, reading the data set and creating features from the data set, and then number two is actually training our model. So, I'm going to go over a high level and then we'll dive into the details. So, this first aspect I talked about is reading the data set. So, what we're going to do is we're going to create a generalizable read_dataset function. In here we'll specify the file name that we want, the mode as in if we're in training or foreign testing, and then also the batch size. So, this is going to read a file name and allow us to read it into a TensorFlow estimator. So, I'm going to keep scrolling down a bit and I'll return back. We also need to define the feature columns, and these are going to be the transformations that we're going to need to make to our dataset. So, then the second part of this lab is once we're able to read our data, we want to actually train and evaluate our model. So, the way that we're going to do that is we're going to create a DNN regressor, why is it a regressor? Or we're trying to predict baby weight, which is a continuous value, so therefore we need to utilize regression. Right, so now we need to specify our train set up which is going to be input function, how we're going to get our data and we're going to use read_dataset and we're going to read in the training.csv file, and we're going to tell it that we're in training mode. This is in contrast to when we're doing evaluation but we're still going to use the read data set function we talked about earlier, but now we're going to use the eval.csv file and we're going to be an eval mode. So, we'll return back to this, but I want to give the high-level view and finally we'll call train and evaluate. Alright, so let's talk about this read data set a little bit more, so in the data set we need to specify what are the columns that we're interested in. Well, these are all the columns that we wrote in our prior CSV file, we need to say what the label column is, the key and then these defaults. What are the default values? Well, this is imputation, so if we don't have these values, we're just going to impute with these exact values. So, for example, we're going to use a null string to just specify we don't have the value. What are these default values? Well, these default values are values that we're going to impute into the data set, and the case that we're actually missing these values. One thing that's interesting is we'll impute it with this null string, a string that just says null. Why will we do this? Well, this will allow us for categorical columns to tell the model that we don't have the value. So, earlier we talked about if you may or may not have ultrasound data, then we might have to build one model but we need to account for the fact that if we don't have ultrasound, we might not have the sex of the baby. So, in this case, we'll have to encode it as a null value. Let's talk about the read data set. We touched by at a high level but let's go into a little more detail about the function. So, the read data sets function is going to be used as the argument to the input function into the TensorFlow estimator, the input function is expecting a function returned. So, not surprisingly the read data set it's actually going to return a function back. It's going to be this underscore input function, so let's dive into this underscore input function. So, at a high level, it's going to read our CSV files and create a data set object. The waves can read our CSV files is using the decode CSV function and this is something that you can use on your own CSV data set, what it's essentially doing is given an input decode it using the default values in the CSV columns that we specified above, then parse out the features and the label which will then be used as inputs into the model, then work on a written list of files that we need to use. In this case it's just going to be a train or eval.csv, so it'll just be one file. But we could read it in the list of files if we wanted. Now, we can just specify if we're in a training mode or evaluation mode. If we're in training, we'll specify the number of epochs as none. Why? Because we want it indefinitely go through the data set and tell our number of steps is reached, so we'll use number of steps as the filter that will stop our training happening. This is opposed to evaluation, where number of epoch is just equal to one because we only need to go through this once during evaluation. With that, we're going to return back our data set iterator so that way we can continually call this if we're in training or just call it once if we're in evaluation. So, let's talk about feature columns. What are feature columns? Feature columns are almost the intermediate layer between our input function and our estimator API in our estimator model. So, we're going read our data from our input function, and then we're going to use the feature columns to potentially apply a transformation to these data. In the case of categorical columns, we want to convert them to numeric values. So what we're going do is we're going to use the tf.feature_column.categorical_column_with_vocabulary_list, and we're going to specify the column and the values that the categories can be. So, it makes sense that we'll do this for a column is_male because this is a string or categorical field and we need to convert it to dummy variables. So, we're going to specify the three possible values it can take. For numeric columns, we're not going to make any transformation to them, but we do need to indicate that they are numeric columns. So, mother_age we're going to tell the estimator that it's a numeric column. We can do these for other features as well. To predict what the model, we also need to build a serving input function. All the inputs that our users going to provide, and the goal of this is going to be or actually serving our model. We need to specify how an end user will submit data or features into our model. What this serving input function is specify in are, in this case, the four different columns that we will be inputting into our model. These will again get run through the feature columns that we defined above. Then, we need to do quick transformation to turn these from vector into a 2D tensor, and then we'll return these ServingInputReceiver, we'll call this later on below. With that, let's do the mean of this lab which is actually going to be training and evaluating our model. So, what we need to specify is we want to specify we're building the DNNRegressor. We need to say the output_dir or the model_dir, and this is really important. What this is telling us is where are we going to save the output of our model including checkpoints, the model graph, and other metadata about our model. We need to get the feature columns that we defined above, specify the shape of our network. In this case, it's going to have 64 and 32 neurons per layer respectively, and we did talk about the configuration. Now we need to say, "Okay, how are we going to train our model?" Well, we're going to train it using the train.csv files that we talked about earlier, we're going to specify the number of training steps that we want to use, and then we need to specify how our model's going to be served, and that's the serving_input function that we talked about above. Finally, for evaluation, we want to use the read_dataset but with the eval.csv. Steps is equal to none because we just have one epochs, so we're only going to go through the dataset ones. Start_delay_secs equals 60, so let's not evaluate until 60 seconds after training. Similarly, with the throttle_secs, let's evaluate every N seconds, we don't want to evaluate too much because evaluation comes at a cost. So, we need to pick some sort of interval where we want to evaluate. With that, we can have our train_and_evaluate. So, what we can do is we can run the model. First, we wanted to remove babyweight_trained. So in case we had that directory, we're going to remove it. So basically, it'll start fresh every time we want to train a model, and then we call our train_and_evaluate function that we just defined up above and save it in a directory called babyweight_trained. So, I can run that right now, and you see it will be saving a model. If we want to look at our model output, we can go to the babyweight_trained folder and we can look at the metadata. This is the data that we're going to ultimately used for serving our model in feature labs. So, you can see the results of our model, and what we can do is, we can actually monitor and experiment using TensorBoard. So, I'm going to go ahead and hit this with shift-enter, and this will allow us to pull up TensorBoard, and we see encouraging results where our average loss is actually decreasing with time. So, this is very encouraging and then telling us that our model is actually learning. That's it for the DNN lab, for training a model. Let's finish up with the wide-and-deep lab. So if we open that up, this code is very similar to what we saw before. We're still going to use the estimator API, we're still going to read the same data, everything's going to be really similar. Let me show you the key difference. The key difference is the feature columns. So, wide-and-deep. We're using the DNN model, deep neural network, and a linear model. With these two models, they're going to have two different types of inputs. The linear model likes to have sparse inputs. So, we're going to specify what are the sparse inputs. The deep model can deal with dense inputs. So first of all, we'll define variables to make it easy to work with for the different columns that we're working with. Then we'll discretize some of our numeric columns. So in this case, we'll create buckets for age and gestation. This is commonly done when you're respecting the nonlinear relationship with your numeric values. In this case, this isn't the case but it's a very useful example to show you how you can use wide-and-deep. So in this case, we have called up categorical columns like is_male and plurality, which are going to be sparser columns. They're either going to be a one or a zero, mostly zero but a one, and the age_buckets and the gestation_buckets, we just created these. So these are going to be sparse columns that we're going to use for our wide model. For our deep model, we want to use mother_age and gestation_weeks, these are continuous values that we have in our dataset, and then we could also create embeddings as well. So, we created an embedding value based on the crosses of the wide which is a really sparse feature vector, but by embedding that, we can just turn it into a thee-dimensional continuous valued feature columns, and then we'll return our wide and our deep features. Finally, now instead of calling a DNNRegressor, we'll call DNNLinearCombinedRegressor. What's the difference? Well, the difference is now we have two feature column inputs, a linear_feature_columns and a dnn_feature_columns. Not surprisingly, the wide ones are going to go into the linear_feature_columns and the deep ones are going to go into the deep feature columns. Beyond that, everything else will be the same as what we've seen before. Similarly, we can actually visualize our results in TensorBoard, and we also see good results here, which is showing us our model is learning as well. So, that's it for training our model. We trained a model to predict what the babyweight will be. We've done this on a local dataset, and we verified our code is working, and we verified that we're actually learning from our data. What this will do is it will pave the way for us to actually package up our code, train it on a large dataset by submitting a job to ML Engine. So, that's where we're marching towards. But before we do that, we have to first prepare our dataset, by preparing a training and evaluation dataset. So let's move on to that. Now that we have finished building our TensorFlow model, let's return back to the dataset and create training and evaluation datasets. We will do this on the full dataset. It's important to train and evaluate on the full dataset because machine learning model performance improves with data size. You don't want to train a model on a small dataset and make assumptions about its performance on that basis. That same model, when trained on a larger dataset, will often be able to reach much better performance. This is especially true in models that have more than one layer.