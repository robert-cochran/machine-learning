1
00:00:00,000 --> 00:00:02,820
Now, we're going to walk through the solution

2
00:00:02,820 --> 00:00:06,160
for the lab you just tried on creating a TensorFlow model.

3
00:00:06,160 --> 00:00:08,245
If you haven't already tried this lab,

4
00:00:08,245 --> 00:00:10,900
go back and try it in Qwiklabs.

5
00:00:10,900 --> 00:00:12,715
If you have already tried it,

6
00:00:12,715 --> 00:00:15,150
know that you have multiple attempts for each lab.

7
00:00:15,150 --> 00:00:17,960
So, if you see something cool in this walkthrough,

8
00:00:17,960 --> 00:00:20,690
feel free to go back and try it again.

9
00:00:20,690 --> 00:00:23,635
I recommend that you build a linear model,

10
00:00:23,635 --> 00:00:25,020
a deep neural network model,

11
00:00:25,020 --> 00:00:26,620
and a wide and deep model,

12
00:00:26,620 --> 00:00:29,125
or you can just build the wide and deep model.

13
00:00:29,125 --> 00:00:33,170
It's good practice though just start out with a simple model if you can.

14
00:00:33,170 --> 00:00:35,910
Plus, pre-built estimators in TensorFlow make

15
00:00:35,910 --> 00:00:39,105
it easy to try out different model architectures.

16
00:00:39,105 --> 00:00:44,125
All right, let's get started and build TensorFlow models.

17
00:00:44,125 --> 00:00:49,890
So, you'll notice that there's actually two files for this third lab.

18
00:00:49,890 --> 00:00:53,390
There is the DNN file and the WD.

19
00:00:53,390 --> 00:00:58,475
The DNN is the Deep Neural Network and the WD stands for Wide and Deep.

20
00:00:58,475 --> 00:01:04,920
Let's walk through the DNN section and then we'll do the wide and deep afterwards.

21
00:01:05,610 --> 00:01:10,470
So, if we go to the third one, for DNN.

22
00:01:11,140 --> 00:01:16,540
Earlier, we spent time creating the sample data set in the prior lab.

23
00:01:16,540 --> 00:01:19,100
Why did we do this?

24
00:01:19,170 --> 00:01:25,585
Well, we wanted to use this small sub-sample of data so we can develop a model locally.

25
00:01:25,585 --> 00:01:27,800
Once we're satisfied with our local model,

26
00:01:27,800 --> 00:01:32,110
we can then scale it out to the cloud to get real performance evaluation.

27
00:01:32,230 --> 00:01:36,045
For now, let's build this local model in TensorFlow.

28
00:01:36,045 --> 00:01:37,670
So as we've been doing before,

29
00:01:37,670 --> 00:01:42,050
we need to specify the bucket in the project and the region of interest.

30
00:01:42,050 --> 00:01:45,310
So, the bucket in the project are specific to you,

31
00:01:45,310 --> 00:01:47,200
in this case, minus crawls sandbox,

32
00:01:47,200 --> 00:01:48,440
for the bucket and the project,

33
00:01:48,440 --> 00:01:52,310
you can set them again as environmental variables,

34
00:01:52,310 --> 00:01:55,745
and we will keep proceeding on.

35
00:01:55,745 --> 00:01:59,895
Oh, you should have your CSV files from the prior lab,

36
00:01:59,895 --> 00:02:02,920
or evaluation CSV and our chain CSV.

37
00:02:02,920 --> 00:02:07,420
We will use these to train our models and evaluate them.

38
00:02:07,420 --> 00:02:16,375
All right, so let's use the TensorFlow estimator API.

39
00:02:16,375 --> 00:02:21,955
So, we're going to import the libraries of interest that we need to complete this lab,

40
00:02:21,955 --> 00:02:25,950
including Shtil, numpy in TensorFlow.

41
00:02:26,990 --> 00:02:34,655
So, there's really two key components to this entire lab right,

42
00:02:34,655 --> 00:02:39,200
there's number one, reading the data set and creating features from the data set,

43
00:02:39,200 --> 00:02:42,665
and then number two is actually training our model.

44
00:02:42,665 --> 00:02:46,900
So, I'm going to go over a high level and then we'll dive into the details.

45
00:02:46,900 --> 00:02:51,640
So, this first aspect I talked about is reading the data set.

46
00:02:51,640 --> 00:02:53,065
So, what we're going to do is we're going to create

47
00:02:53,065 --> 00:02:57,055
a generalizable read_dataset function.

48
00:02:57,055 --> 00:02:59,840
In here we'll specify the file name that we want,

49
00:02:59,840 --> 00:03:04,135
the mode as in if we're in training or foreign testing,

50
00:03:04,135 --> 00:03:06,135
and then also the batch size.

51
00:03:06,135 --> 00:03:08,430
So, this is going to read

52
00:03:08,430 --> 00:03:13,270
a file name and allow us to read it into a TensorFlow estimator.

53
00:03:13,270 --> 00:03:16,885
So, I'm going to keep scrolling down a bit and I'll return back.

54
00:03:16,885 --> 00:03:19,630
We also need to define the feature columns,

55
00:03:19,630 --> 00:03:20,650
and these are going to be

56
00:03:20,650 --> 00:03:24,140
the transformations that we're going to need to make to our dataset.

57
00:03:26,990 --> 00:03:32,810
So, then the second part of this lab is once we're able to read our data,

58
00:03:32,810 --> 00:03:36,465
we want to actually train and evaluate our model.

59
00:03:36,465 --> 00:03:40,915
So, the way that we're going to do that is we're going to create a DNN regressor,

60
00:03:40,915 --> 00:03:43,110
why is it a regressor?

61
00:03:43,110 --> 00:03:45,155
Or we're trying to predict baby weight,

62
00:03:45,155 --> 00:03:46,460
which is a continuous value,

63
00:03:46,460 --> 00:03:49,520
so therefore we need to utilize regression.

64
00:03:50,670 --> 00:03:56,355
Right, so now we need to specify our train set up which is going to be input function,

65
00:03:56,355 --> 00:03:58,560
how we're going to get our data and we're going to use

66
00:03:58,560 --> 00:04:03,550
read_dataset and we're going to read in the training.csv file,

67
00:04:03,550 --> 00:04:06,935
and we're going to tell it that we're in training mode.

68
00:04:06,935 --> 00:04:09,430
This is in contrast to when we're doing

69
00:04:09,430 --> 00:04:11,230
evaluation but we're still going to use

70
00:04:11,230 --> 00:04:13,660
the read data set function we talked about earlier,

71
00:04:13,660 --> 00:04:19,750
but now we're going to use the eval.csv file and we're going to be an eval mode.

72
00:04:19,750 --> 00:04:21,670
So, we'll return back to this,

73
00:04:21,670 --> 00:04:28,640
but I want to give the high-level view and finally we'll call train and evaluate.

74
00:04:31,730 --> 00:04:36,210
Alright, so let's talk about this read data set a little bit more,

75
00:04:36,210 --> 00:04:41,025
so in the data set we need to specify what are the columns that we're interested in.

76
00:04:41,025 --> 00:04:45,210
Well, these are all the columns that we wrote in our prior CSV file,

77
00:04:45,210 --> 00:04:48,280
we need to say what the label column is,

78
00:04:48,280 --> 00:04:51,415
the key and then these defaults.

79
00:04:51,415 --> 00:04:52,990
What are the default values?

80
00:04:52,990 --> 00:04:56,350
Well, this is imputation,

81
00:04:56,350 --> 00:04:57,635
so if we don't have these values,

82
00:04:57,635 --> 00:05:00,255
we're just going to impute with these exact values.

83
00:05:00,255 --> 00:05:01,750
So, for example, we're going to use

84
00:05:01,750 --> 00:05:06,155
a null string to just specify we don't have the value.

85
00:05:06,155 --> 00:05:08,230
What are these default values?

86
00:05:08,230 --> 00:05:11,770
Well, these default values are values that we're going to impute into the data set,

87
00:05:11,770 --> 00:05:14,405
and the case that we're actually missing these values.

88
00:05:14,405 --> 00:05:18,520
One thing that's interesting is we'll impute it with this null string,

89
00:05:18,520 --> 00:05:20,210
a string that just says null.

90
00:05:20,210 --> 00:05:21,735
Why will we do this?

91
00:05:21,735 --> 00:05:23,350
Well, this will allow us for

92
00:05:23,350 --> 00:05:27,110
categorical columns to tell the model that we don't have the value.

93
00:05:27,110 --> 00:05:31,420
So, earlier we talked about if you may or may not have ultrasound data,

94
00:05:31,420 --> 00:05:33,910
then we might have to build one model but

95
00:05:33,910 --> 00:05:36,070
we need to account for the fact that if we don't have ultrasound,

96
00:05:36,070 --> 00:05:37,900
we might not have the sex of the baby.

97
00:05:37,900 --> 00:05:38,920
So, in this case,

98
00:05:38,920 --> 00:05:42,430
we'll have to encode it as a null value.

99
00:05:42,430 --> 00:05:44,885
Let's talk about the read data set.

100
00:05:44,885 --> 00:05:49,825
We touched by at a high level but let's go into a little more detail about the function.

101
00:05:49,825 --> 00:05:53,170
So, the read data sets function is going to be used as

102
00:05:53,170 --> 00:05:57,045
the argument to the input function into the TensorFlow estimator,

103
00:05:57,045 --> 00:06:00,640
the input function is expecting a function returned.

104
00:06:00,640 --> 00:06:07,065
So, not surprisingly the read data set it's actually going to return a function back.

105
00:06:07,065 --> 00:06:10,095
It's going to be this underscore input function,

106
00:06:10,095 --> 00:06:13,780
so let's dive into this underscore input function.

107
00:06:13,780 --> 00:06:19,900
So, at a high level, it's going to read our CSV files and create a data set object.

108
00:06:19,900 --> 00:06:22,820
The waves can read our CSV files is using

109
00:06:22,820 --> 00:06:29,965
the decode CSV function and this is something that you can use on your own CSV data set,

110
00:06:29,965 --> 00:06:34,520
what it's essentially doing is given an input

111
00:06:34,520 --> 00:06:40,220
decode it using the default values in the CSV columns that we specified above,

112
00:06:40,220 --> 00:06:41,690
then parse out the features and

113
00:06:41,690 --> 00:06:46,275
the label which will then be used as inputs into the model,

114
00:06:46,275 --> 00:06:49,690
then work on a written list of files that we need to use.

115
00:06:49,690 --> 00:06:52,805
In this case it's just going to be a train or eval.csv,

116
00:06:52,805 --> 00:06:54,500
so it'll just be one file.

117
00:06:54,500 --> 00:07:02,500
But we could read it in the list of files if we wanted.

118
00:07:02,500 --> 00:07:06,845
Now, we can just specify if we're in a training mode or evaluation mode.

119
00:07:06,845 --> 00:07:08,670
If we're in training,

120
00:07:08,670 --> 00:07:11,375
we'll specify the number of epochs as none.

121
00:07:11,375 --> 00:07:14,000
Why? Because we want it indefinitely go through

122
00:07:14,000 --> 00:07:16,850
the data set and tell our number of steps is reached,

123
00:07:16,850 --> 00:07:22,760
so we'll use number of steps as the filter that will stop our training happening.

124
00:07:22,760 --> 00:07:25,440
This is opposed to evaluation,

125
00:07:25,440 --> 00:07:28,310
where number of epoch is just equal to one because we only need to go

126
00:07:28,310 --> 00:07:31,895
through this once during evaluation.

127
00:07:31,895 --> 00:07:36,870
With that, we're going to return back our data set iterator

128
00:07:36,870 --> 00:07:39,250
so that way we can continually call this if we're in

129
00:07:39,250 --> 00:07:43,130
training or just call it once if we're in evaluation.

130
00:07:53,280 --> 00:07:56,955
So, let's talk about feature columns.

131
00:07:56,955 --> 00:07:59,565
What are feature columns?

132
00:07:59,565 --> 00:08:03,000
Feature columns are almost the intermediate layer between

133
00:08:03,000 --> 00:08:08,180
our input function and our estimator API in our estimator model.

134
00:08:08,180 --> 00:08:13,395
So, we're going read our data from our input function,

135
00:08:13,395 --> 00:08:15,320
and then we're going to use the feature columns to

136
00:08:15,320 --> 00:08:18,615
potentially apply a transformation to these data.

137
00:08:18,615 --> 00:08:21,370
In the case of categorical columns,

138
00:08:21,370 --> 00:08:25,620
we want to convert them to numeric values.

139
00:08:25,620 --> 00:08:26,650
So what we're going do is we're going

140
00:08:26,650 --> 00:08:31,200
to use the tf.feature_column.categorical_column_with_vocabulary_list,

141
00:08:31,200 --> 00:08:35,325
and we're going to specify the column and the values that the categories can be.

142
00:08:35,325 --> 00:08:40,170
So, it makes sense that we'll do this for a column is_male because this is

143
00:08:40,170 --> 00:08:44,550
a string or categorical field and we need to convert it to dummy variables.

144
00:08:44,550 --> 00:08:48,120
So, we're going to specify the three possible values it can take.

145
00:08:48,120 --> 00:08:51,685
For numeric columns, we're not going to make any transformation to them,

146
00:08:51,685 --> 00:08:54,470
but we do need to indicate that they are numeric columns.

147
00:08:54,470 --> 00:09:00,315
So, mother_age we're going to tell the estimator that it's a numeric column.

148
00:09:00,315 --> 00:09:03,870
We can do these for other features as well.

149
00:09:04,780 --> 00:09:10,225
To predict what the model, we also need to build a serving input function.

150
00:09:10,225 --> 00:09:14,060
All the inputs that our users going to provide,

151
00:09:14,060 --> 00:09:18,105
and the goal of this is going to be or actually serving our model.

152
00:09:18,105 --> 00:09:25,315
We need to specify how an end user will submit data or features into our model.

153
00:09:25,315 --> 00:09:29,250
What this serving input function is specify in are,

154
00:09:29,250 --> 00:09:35,275
in this case, the four different columns that we will be inputting into our model.

155
00:09:35,275 --> 00:09:39,575
These will again get run through the feature columns that we defined above.

156
00:09:39,575 --> 00:09:46,255
Then, we need to do quick transformation to turn these from vector into a 2D tensor,

157
00:09:46,255 --> 00:09:50,310
and then we'll return these ServingInputReceiver,

158
00:09:50,310 --> 00:09:52,730
we'll call this later on below.

159
00:09:55,460 --> 00:09:59,330
With that, let's do

160
00:09:59,330 --> 00:10:05,140
the mean of this lab which is actually going to be training and evaluating our model.

161
00:10:05,810 --> 00:10:12,205
So, what we need to specify is we want to specify we're building the DNNRegressor.

162
00:10:12,205 --> 00:10:15,575
We need to say the output_dir or the model_dir,

163
00:10:15,575 --> 00:10:17,300
and this is really important.

164
00:10:17,300 --> 00:10:20,200
What this is telling us is where are we going to

165
00:10:20,200 --> 00:10:23,660
save the output of our model including checkpoints,

166
00:10:23,660 --> 00:10:27,815
the model graph, and other metadata about our model.

167
00:10:27,815 --> 00:10:31,170
We need to get the feature columns that we defined above,

168
00:10:31,170 --> 00:10:33,350
specify the shape of our network.

169
00:10:33,350 --> 00:10:38,195
In this case, it's going to have 64 and 32 neurons per layer respectively,

170
00:10:38,195 --> 00:10:41,290
and we did talk about the configuration.

171
00:10:43,300 --> 00:10:45,540
Now we need to say,

172
00:10:45,540 --> 00:10:47,555
"Okay, how are we going to train our model?"

173
00:10:47,555 --> 00:10:53,290
Well, we're going to train it using the train.csv files that we talked about earlier,

174
00:10:53,290 --> 00:10:57,625
we're going to specify the number of training steps that we want to use,

175
00:10:57,625 --> 00:11:02,325
and then we need to specify how our model's going to be served,

176
00:11:02,325 --> 00:11:05,395
and that's the serving_input function that we talked about above.

177
00:11:05,395 --> 00:11:12,000
Finally, for evaluation, we want to use the read_dataset but with the eval.csv.

178
00:11:12,000 --> 00:11:14,950
Steps is equal to none because we just have one epochs,

179
00:11:14,950 --> 00:11:16,925
so we're only going to go through the dataset ones.

180
00:11:16,925 --> 00:11:23,695
Start_delay_secs equals 60, so let's not evaluate until 60 seconds after training.

181
00:11:23,695 --> 00:11:25,795
Similarly, with the throttle_secs,

182
00:11:25,795 --> 00:11:28,630
let's evaluate every N seconds,

183
00:11:28,630 --> 00:11:32,035
we don't want to evaluate too much because evaluation comes at a cost.

184
00:11:32,035 --> 00:11:37,370
So, we need to pick some sort of interval where we want to evaluate.

185
00:11:37,640 --> 00:11:42,020
With that, we can have our train_and_evaluate.

186
00:11:42,760 --> 00:11:46,015
So, what we can do is we can run the model.

187
00:11:46,015 --> 00:11:49,440
First, we wanted to remove babyweight_trained.

188
00:11:49,440 --> 00:11:51,400
So in case we had that directory, we're going to remove it.

189
00:11:51,400 --> 00:11:54,815
So basically, it'll start fresh every time we want to train a model,

190
00:11:54,815 --> 00:11:57,810
and then we call our train_and_evaluate function that we just defined up

191
00:11:57,810 --> 00:12:01,975
above and save it in a directory called babyweight_trained.

192
00:12:01,975 --> 00:12:05,225
So, I can run that right now,

193
00:12:05,225 --> 00:12:08,055
and you see it will be saving a model.

194
00:12:08,055 --> 00:12:10,040
If we want to look at our model output,

195
00:12:10,040 --> 00:12:15,660
we can go to the babyweight_trained folder and we can look at the metadata.

196
00:12:15,660 --> 00:12:18,320
This is the data that we're going to ultimately used

197
00:12:18,320 --> 00:12:22,010
for serving our model in feature labs.

198
00:12:24,770 --> 00:12:29,660
So, you can see the results of our model,

199
00:12:32,940 --> 00:12:38,330
and what we can do is, we can actually monitor and experiment using TensorBoard.

200
00:12:38,330 --> 00:12:41,110
So, I'm going to go ahead and hit this with shift-enter,

201
00:12:41,110 --> 00:12:44,715
and this will allow us to pull up TensorBoard,

202
00:12:44,715 --> 00:12:53,900
and we see encouraging results where our average loss is actually decreasing with time.

203
00:12:53,900 --> 00:12:59,220
So, this is very encouraging and then telling us that our model is actually learning.

204
00:13:05,390 --> 00:13:09,975
That's it for the DNN lab,

205
00:13:09,975 --> 00:13:11,955
for training a model.

206
00:13:11,955 --> 00:13:15,585
Let's finish up with the wide-and-deep lab.

207
00:13:15,585 --> 00:13:17,365
So if we open that up,

208
00:13:17,365 --> 00:13:19,980
this code is very similar to what we saw before.

209
00:13:19,980 --> 00:13:22,040
We're still going to use the estimator API,

210
00:13:22,040 --> 00:13:23,615
we're still going to read the same data,

211
00:13:23,615 --> 00:13:24,980
everything's going to be really similar.

212
00:13:24,980 --> 00:13:26,995
Let me show you the key difference.

213
00:13:26,995 --> 00:13:32,930
The key difference is the feature columns.

214
00:13:32,930 --> 00:13:34,430
So, wide-and-deep.

215
00:13:34,430 --> 00:13:36,150
We're using the DNN model,

216
00:13:36,150 --> 00:13:39,380
deep neural network, and a linear model.

217
00:13:39,610 --> 00:13:44,235
With these two models, they're going to have two different types of inputs.

218
00:13:44,235 --> 00:13:47,870
The linear model likes to have sparse inputs.

219
00:13:47,870 --> 00:13:50,940
So, we're going to specify what are the sparse inputs.

220
00:13:50,940 --> 00:13:54,775
The deep model can deal with dense inputs.

221
00:13:54,775 --> 00:13:58,650
So first of all, we'll define variables to make

222
00:13:58,650 --> 00:14:02,715
it easy to work with for the different columns that we're working with.

223
00:14:02,715 --> 00:14:06,680
Then we'll discretize some of our numeric columns.

224
00:14:06,680 --> 00:14:10,230
So in this case, we'll create buckets for age and gestation.

225
00:14:10,230 --> 00:14:12,040
This is commonly done when you're respecting

226
00:14:12,040 --> 00:14:14,715
the nonlinear relationship with your numeric values.

227
00:14:14,715 --> 00:14:17,760
In this case, this isn't the case but it's

228
00:14:17,760 --> 00:14:22,245
a very useful example to show you how you can use wide-and-deep.

229
00:14:22,245 --> 00:14:28,135
So in this case, we have called up categorical columns like is_male and plurality,

230
00:14:28,135 --> 00:14:29,885
which are going to be sparser columns.

231
00:14:29,885 --> 00:14:31,555
They're either going to be a one or a zero,

232
00:14:31,555 --> 00:14:34,080
mostly zero but a one,

233
00:14:34,080 --> 00:14:36,260
and the age_buckets and the gestation_buckets,

234
00:14:36,260 --> 00:14:37,365
we just created these.

235
00:14:37,365 --> 00:14:40,590
So these are going to be sparse columns that we're going to use for our wide model.

236
00:14:40,590 --> 00:14:45,475
For our deep model, we want to use mother_age and gestation_weeks,

237
00:14:45,475 --> 00:14:48,860
these are continuous values that we have in our dataset,

238
00:14:48,860 --> 00:14:51,820
and then we could also create embeddings as well.

239
00:14:51,820 --> 00:14:57,300
So, we created an embedding value based on the crosses of the wide

240
00:14:57,300 --> 00:15:02,330
which is a really sparse feature vector, but by embedding that,

241
00:15:02,330 --> 00:15:07,660
we can just turn it into a thee-dimensional continuous valued feature columns,

242
00:15:07,660 --> 00:15:13,660
and then we'll return our wide and our deep features.

243
00:15:14,090 --> 00:15:24,435
Finally, now instead of calling a DNNRegressor, we'll call DNNLinearCombinedRegressor.

244
00:15:24,435 --> 00:15:26,210
What's the difference?

245
00:15:26,210 --> 00:15:29,915
Well, the difference is now we have two feature column inputs,

246
00:15:29,915 --> 00:15:33,825
a linear_feature_columns and a dnn_feature_columns.

247
00:15:33,825 --> 00:15:37,450
Not surprisingly, the wide ones are going to go into the linear_feature_columns

248
00:15:37,450 --> 00:15:42,265
and the deep ones are going to go into the deep feature columns.

249
00:15:42,265 --> 00:15:47,200
Beyond that, everything else will be the same as what we've seen before.

250
00:15:47,200 --> 00:15:51,800
Similarly, we can actually visualize our results in TensorBoard,

251
00:15:51,800 --> 00:15:55,880
and we also see good results here,

252
00:15:55,880 --> 00:15:58,665
which is showing us our model is learning as well.

253
00:15:58,665 --> 00:16:01,870
So, that's it for training our model.

254
00:16:01,870 --> 00:16:05,020
We trained a model to predict what the babyweight will be.

255
00:16:05,020 --> 00:16:06,905
We've done this on a local dataset,

256
00:16:06,905 --> 00:16:08,710
and we verified our code is working,

257
00:16:08,710 --> 00:16:12,345
and we verified that we're actually learning from our data.

258
00:16:12,345 --> 00:16:17,385
What this will do is it will pave the way for us to actually package up our code,

259
00:16:17,385 --> 00:16:22,225
train it on a large dataset by submitting a job to ML Engine.

260
00:16:22,225 --> 00:16:24,340
So, that's where we're marching towards.

261
00:16:24,340 --> 00:16:25,710
But before we do that,

262
00:16:25,710 --> 00:16:28,060
we have to first prepare our dataset,

263
00:16:28,060 --> 00:16:30,555
by preparing a training and evaluation dataset.

264
00:16:30,555 --> 00:16:32,150
So let's move on to that.

265
00:16:32,150 --> 00:16:35,260
Now that we have finished building our TensorFlow model,

266
00:16:35,260 --> 00:16:39,280
let's return back to the dataset and create training and evaluation datasets.

267
00:16:39,280 --> 00:16:41,755
We will do this on the full dataset.

268
00:16:41,755 --> 00:16:45,560
It's important to train and evaluate on the full dataset because

269
00:16:45,560 --> 00:16:49,320
machine learning model performance improves with data size.

270
00:16:49,320 --> 00:16:52,130
You don't want to train a model on a small dataset and make

271
00:16:52,130 --> 00:16:54,835
assumptions about its performance on that basis.

272
00:16:54,835 --> 00:16:58,080
That same model, when trained on a larger dataset,

273
00:16:58,080 --> 00:17:01,265
will often be able to reach much better performance.

274
00:17:01,265 --> 00:17:05,690
This is especially true in models that have more than one layer.