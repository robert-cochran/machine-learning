So, the model training is key but we're not quite finished yet. We still need to serve our model. After all, what is the point of building an ML model unless our end users can actually consume our model? In the previous lab, we stored our model parameters, graphs, and other model information using the model dir argument. This save the model information containing our serving input function and preprocessing pipeline along with the actual model logic. Now, we will point Cloud ML Engine to this model directory so we can make predictions using our trained model. The end users, i.e. the clients, will be able to consume our model via a REST API call with input variables. You can't reuse the training input function that we built earlier for serving. As a reminder, the serving input function is what will be used to parse what comes from the user at predict time. In the serving input function, we might need to parse different input formats than was used for training. For example, training input could be in CSV while serving could be expecting JSON. Also, serving input function doesn't need labels. Finally, features received from the user might be a smaller set of variables we didn't use during training. For example, train with hour of day, day of week as inputs to the neural network. But during prediction, we can get these from the system clock so the user doesn't need to supply them. In the prior lab, we defined a serving function in the babyweight trainer model.py file. This file is where we define the API for serving our model. In addition, it can also contain preprocessing logic as well. For example, per the previous example, if we need to get the current time and add the day of week, Monday, Wednesday, et cetera, this is where it would go. Once we have built our serving API via the serving function, we're now ready to deploy our trained model to GCP. As you can see, this is done in a couple lines of code. In this case, we're deploying a model named taxifare and we're deploying the first version of it, V1. We specify the model location. Remember, this was the model dir that we specified during model training and we point our model to this exact location. Then we run gcloud ml-engine models create, which creates the model that we specified. Then we create the version pointing it to the model location that we have above. Once you do that, that's it. We've deployed our model and we can now get predictions from it. So, the client code can make REST calls and this is done in a couple of lines of code. So, what we need to do is first of all, we need to get the credentials for the user. We need to run API equals discovery.build, which creates a service object where we define the Google API and the version we want to use. In this case, we want to use ML Engine and version one. We then want to create example data. We will create this in JSON format and it will contain example features that we will use to get our prediction. We will then call the API projects.predict, point it to the model that we just deployed earlier, and also point it to this request data that we've created contain example features. This response that came back will contain the predictions that our model made.