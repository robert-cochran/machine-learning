So, in this lab, we are going to basically create a dataset but where last time we used Pandas and we got down to 12,000 rows or something like that, this time we're going to pre-process the entire dataset from BigQuery and create a training dataset and evaluation dataset for training. In order to be able to do that, we're going to use Cloud Dataflow. I now have the fourth Notebook open and, again, I need to change the project and the bucket but what I'll do is I'll cheat. I'm using the same Qwiklabs project so I'll just go ahead and copy the project bucket and region from my previous Notebook number two. At this point, I have those three things set, my Bash script, my bucket is created, and let's go ahead and now let's look at the dataflow job. What is a dataflow job doing? It starts here and it is reading from BigQuery, starting with a selection query, converting it to CSV, and writing the output as CSV files, as text files. So, let's see what's the first task. The first task is to do this conversion, it's basically calling the to_csv function, passing in a rowdict, the rowdict is what is coming in from BigQuery and we need to basically take those and do all of those cleanup kinds of things that we did. So, for example, we might say csv_fields is empty list for now and we might say csv_fields of zero is the same as rowdict of is_male. Then it's basically taking the rowdict data and copying it directly into CSV, actually, it wouldn't be like this, CSV of csv.append, we would do, and we could do that. But if we do that, we're now copying the data as is. What we really want to do is to also do transformations so we could say csv_fields of zero is, for example, the rowdict of is_male, that's the original value. If it is true, then we want to basically make it, say, 0.5 or otherwise its negative 0.5. So, we're basically converting a Boolean into a floating point number. The question really is what kinds of changes, what kinds of transformations did you do in your Pandas that you want to repeat in dataflow? So, in my case, some of the things that I did was to create the with ultrasound and without ultrasound so I know that I'm going to need to import a copy and I need to basically make a second copy of the rowdict, so let me just go ahead and do this. So, I basically have two copies of the rowdict. There is a no_ultrasound, which basically copies a rowdict, and the w_ultrasound, which is another copy of the rowdict. Now, I can go ahead and change the no_ultrasound and w_ultrasound to be dictionaries that have the appropriate data. So, what do I mean by this? The w_ultrasound is the original perfect information, the no_ultrasound is the one that has changes in it. So, for example, in the no_ultrasound case, the is_male field is always unknown whereas in the w_ultrasound case, the is_male field is what it was originally in the data. We are not actually converting it to minus one to one or anything like that, this one is just an example of the kinds of things that you might want to do. Let's go ahead and remove this. So, we need to create our csv_fields from these dictionaries and we need to think about what our columns are. So, in my case, in this solution, the columns consist of these fields. So, these are the columns. So, I have here weight_ pounds, is_male, mother_ age, plurality, and gestation_weeks. So, now, let's just think about what happens to the weight in pounds for both of these cases? Nothing, it remains the same. The is_male we know in the no_ultrasound case is unknown, mother_age remains the same, plurality though is a number, it's one, two, three, four, or five in the data and we want to basically convert them into strings. So, what we might do is to basically say, so let's just walk through the code here, so the code here is basically saying that in the no_ultrasound case, the plurality is either single or it's multiple. However, in the w_ultrasound case, we're just converting them into strings. So, we're basically taking the rowdict of plurality and if it is one, then we basically get the zeros which should be single, if it is two, it would be twins, if it's three, it'll be triplets, et cetera. So, that's essentially the transformation. This is the transformation for is_male, this is the transformation of plurality and having done that, is there anything else we need to do? No. So, we can now go ahead and create our csv_fields and yield all of them. So, what we can do is we can say for dictionary in, there are two dictionaries that we have, no_ultrasound and w_ultrasound. In both of these, what we want to do is to basically get all the values and that would basically give us the column values, and once we take all of those values, we can basically go ahead and join them. So, here's where I'm doing this, let me just put this in here and we can walk through the code. So, let's ignore the key for now, we don't really need the key. So, what I'm doing is that for every CSV column, so I want them in this particular order, so for k in CSV_COLUMNS, I'm basically taking the actual if k is in result. So, this is result, if k is in result, so if this column value is present, remember that we ran into a lot of cases where the data value was null, so this verifies that. If it is there, then you're basically putting that value, otherwise you're putting the value none and you're basically making everything a string so that we can basically do a join with it. So, this is now our data and we can now go ahead and return the data. But in this case, what I'm also doing is that I'm maintaining that besides the normal CSV columns, I'm also maintaining a key and I'm forming the key by doing a hash of the exact value and storing it. That way, each row will have a unique key. This is totally optional, you don't need it yet, but one of the things is that when you're doing productionization and you want to do batch prediction, it turns out that you will distribute your predictions over multiple machines and it's super helpful to have a unique key associated with every row so that when the results come back, you know which result, which prediction corresponded to which input. That's what the key is there for. Ignore it because it's not something that you would normally need, but you will need it if you're going to be doing distributed batch prediction and I've gotten into the habit of always doing it. So, I just do it regardless of whether I need batch prediction or not. I always hash all of my columns, form a key, attach the key to my data so it's there if I ever need to do it. So, that's my first to-do. The second to-do is to modify the query to pull up the fields that you want. So, let's look at these fields, we needed to get weight_pounds, is_male, mother_age, plurality, and gestation_weeks. So, I have all the columns that I want and I'm making sure that those columns actually exist. If it's in test mode, I'm basically getting the first 100 and I'm creating a training and evaluation dataset and then I can go ahead and run this to make sure that it actually runs correctly. Once it runs correctly, I can go ahead and change the in_test_mode from locally to the hashlib. I'm using hashlib, I'm not importing it. So, let me go ahead and add import hashlib, and then run it again. This time, it seems to be working and so it's now done. So, my code worked, I can go ahead and check the file that got produced. What was the output file? My output file in_test_mode is /preproc, so that's where I'm writing it to. So, let's go ahead and check if this thing got actually written. So, I'll go ahead and do this, I'll do!ls preproc, and inside my preprocessing directory, there I've created my eval.csv and train.csv. Let's make sure that those files actually look correct. I can do!head preproc/train.csv*. So, now I basically have columns, so that is the weight_pounds, this is the is_male or not, the age, the plurality, the mothers, one of them is a mother_age, the other one is gestation_weeks. Then you have the unique hash associated with that particular row. As I said, this would be useful if we ever need to do batch predictions and batch evaluations, et cetera. So, we know that now our train.csv is correct, we've tested things locally. I can now go back and change the in_test_mode to be false and notice that when we change the test mode to be false, we will launch a dataflow job and the output directory will show up in our bucket in a folder called babyweight. So, now if I run this cell, this is going to launch off a dataflow job and I can go back to my GCP Console, go into Dataflow, so here's Dataflow, and you should see that this batch job has started for preprocessing and we can look at this thing and it's going to basically read from BigQuery, convert it to CSV, write it out. This would take maybe 15, 20 minutes and you can watch the number of CPUs spinning up as they are actually executing. Wait for the job to be done and once the job is done, you should be able to go into this directory just like I did ls on the local preproc directory, you can do ls on the cloud directory to make sure that the eval files and your train files exist. At this point, you basically have the files that you need that contain all of the data so that you can train your model in real time. At this point, we now have the training and evaluation datasets created at scale. The process is also fully automated. We can simply re-run the pipeline periodically to create a new training dataset on fresher data. The next step is to train the model on the larger dataset. While we could train on our 10,000 sample dataset on the Notebook itself, when it comes to larger datasets, we will distribute the training off to the Cloud using ML engine. Lets review that next.