1
00:00:00,000 --> 00:00:01,660
So, in this lab,

2
00:00:01,660 --> 00:00:05,950
we are going to basically create a dataset but where last

3
00:00:05,950 --> 00:00:11,750
time we used Pandas and we got down to 12,000 rows or something like that,

4
00:00:11,750 --> 00:00:16,170
this time we're going to pre-process the entire dataset from

5
00:00:16,170 --> 00:00:22,070
BigQuery and create a training dataset and evaluation dataset for training.

6
00:00:22,070 --> 00:00:23,750
In order to be able to do that,

7
00:00:23,750 --> 00:00:25,660
we're going to use Cloud Dataflow.

8
00:00:25,660 --> 00:00:30,180
I now have the fourth Notebook open and,

9
00:00:30,180 --> 00:00:34,685
again, I need to change the project and the bucket but what I'll do is I'll cheat.

10
00:00:34,685 --> 00:00:38,060
I'm using the same Qwiklabs project so I'll just go ahead and

11
00:00:38,060 --> 00:00:43,030
copy the project bucket and region from my previous Notebook number two.

12
00:00:43,030 --> 00:00:49,455
At this point, I have those three things set, my Bash script,

13
00:00:49,455 --> 00:00:51,425
my bucket is created,

14
00:00:51,425 --> 00:00:58,625
and let's go ahead and now let's look at the dataflow job.

15
00:00:58,625 --> 00:01:00,495
What is a dataflow job doing?

16
00:01:00,495 --> 00:01:05,810
It starts here and it is reading from BigQuery,

17
00:01:05,810 --> 00:01:07,855
starting with a selection query,

18
00:01:07,855 --> 00:01:09,800
converting it to CSV,

19
00:01:09,800 --> 00:01:15,045
and writing the output as CSV files, as text files.

20
00:01:15,045 --> 00:01:17,920
So, let's see what's the first task.

21
00:01:17,920 --> 00:01:22,180
The first task is to do this conversion,

22
00:01:22,180 --> 00:01:26,935
it's basically calling the to_csv function,

23
00:01:26,935 --> 00:01:28,580
passing in a rowdict,

24
00:01:28,580 --> 00:01:32,270
the rowdict is what is coming in from BigQuery and we need to basically

25
00:01:32,270 --> 00:01:36,510
take those and do all of those cleanup kinds of things that we did.

26
00:01:36,510 --> 00:01:44,810
So, for example, we might say csv_fields is empty list for now

27
00:01:44,810 --> 00:01:54,455
and we might say csv_fields of zero is the same as rowdict of is_male.

28
00:01:54,455 --> 00:02:00,480
Then it's basically taking the rowdict data and copying it directly into CSV, actually,

29
00:02:00,480 --> 00:02:03,825
it wouldn't be like this, CSV of csv.append,

30
00:02:03,825 --> 00:02:06,700
we would do, and we could do that.

31
00:02:06,700 --> 00:02:08,190
But if we do that,

32
00:02:08,190 --> 00:02:10,365
we're now copying the data as is.

33
00:02:10,365 --> 00:02:14,490
What we really want to do is to also do transformations so we

34
00:02:14,490 --> 00:02:21,360
could say csv_fields of zero is,

35
00:02:21,360 --> 00:02:25,220
for example, the rowdict of is_male,

36
00:02:25,220 --> 00:02:26,930
that's the original value.

37
00:02:26,930 --> 00:02:29,085
If it is true,

38
00:02:29,085 --> 00:02:33,765
then we want to basically make it, say,

39
00:02:33,765 --> 00:02:38,745
0.5 or otherwise its negative 0.5.

40
00:02:38,745 --> 00:02:43,855
So, we're basically converting a Boolean into a floating point number.

41
00:02:43,855 --> 00:02:48,960
The question really is what kinds of changes,

42
00:02:48,960 --> 00:02:51,120
what kinds of transformations did you

43
00:02:51,120 --> 00:02:57,360
do in your Pandas that you want to repeat in dataflow?

44
00:02:57,360 --> 00:02:58,495
So, in my case,

45
00:02:58,495 --> 00:03:01,210
some of the things that I did was to create the with

46
00:03:01,210 --> 00:03:04,510
ultrasound and without ultrasound so I know that I'm going to

47
00:03:04,510 --> 00:03:11,670
need to import a copy and I need to basically make a second copy of the rowdict,

48
00:03:11,670 --> 00:03:15,540
so let me just go ahead and do this.

49
00:03:15,540 --> 00:03:20,015
So, I basically have two copies of the rowdict.

50
00:03:20,015 --> 00:03:21,850
There is a no_ultrasound,

51
00:03:21,850 --> 00:03:24,035
which basically copies a rowdict,

52
00:03:24,035 --> 00:03:26,895
and the w_ultrasound, which is another copy of the rowdict.

53
00:03:26,895 --> 00:03:29,435
Now, I can go ahead and change the no_ultrasound and

54
00:03:29,435 --> 00:03:34,280
w_ultrasound to be dictionaries that have the appropriate data.

55
00:03:34,280 --> 00:03:35,800
So, what do I mean by this?

56
00:03:35,800 --> 00:03:39,695
The w_ultrasound is the original perfect information,

57
00:03:39,695 --> 00:03:43,390
the no_ultrasound is the one that has changes in it.

58
00:03:43,390 --> 00:03:50,160
So, for example, in the no_ultrasound case,

59
00:03:50,160 --> 00:03:57,435
the is_male field is always unknown whereas in the w_ultrasound case,

60
00:03:57,435 --> 00:04:00,985
the is_male field is what it was originally in the data.

61
00:04:00,985 --> 00:04:06,370
We are not actually converting it to minus one to one or anything like that,

62
00:04:06,370 --> 00:04:08,290
this one is just an example of the kinds of things

63
00:04:08,290 --> 00:04:10,440
that you might want to do. Let's go ahead and remove this.

64
00:04:10,440 --> 00:04:13,510
So, we need to create our csv_fields from

65
00:04:13,510 --> 00:04:18,335
these dictionaries and we need to think about what our columns are.

66
00:04:18,335 --> 00:04:22,045
So, in my case, in this solution,

67
00:04:22,045 --> 00:04:26,925
the columns consist of these fields.

68
00:04:26,925 --> 00:04:29,295
So, these are the columns.

69
00:04:29,295 --> 00:04:32,570
So, I have here weight_ pounds,

70
00:04:32,570 --> 00:04:35,640
is_male, mother_ age, plurality, and gestation_weeks.

71
00:04:35,640 --> 00:04:38,470
So, now, let's just think about what happens to

72
00:04:38,470 --> 00:04:41,470
the weight in pounds for both of these cases?

73
00:04:41,470 --> 00:04:43,105
Nothing, it remains the same.

74
00:04:43,105 --> 00:04:47,225
The is_male we know in the no_ultrasound case is unknown,

75
00:04:47,225 --> 00:04:49,265
mother_age remains the same,

76
00:04:49,265 --> 00:04:51,440
plurality though is a number,

77
00:04:51,440 --> 00:04:52,450
it's one, two, three, four,

78
00:04:52,450 --> 00:04:56,760
or five in the data and we want to basically convert them into strings.

79
00:04:56,760 --> 00:05:00,870
So, what we might do is to basically say,

80
00:05:00,870 --> 00:05:04,960
so let's just walk through the code here,

81
00:05:06,650 --> 00:05:12,560
so the code here is basically saying that in the no_ultrasound case,

82
00:05:12,560 --> 00:05:16,775
the plurality is either single or it's multiple.

83
00:05:16,775 --> 00:05:20,345
However, in the w_ultrasound case,

84
00:05:20,345 --> 00:05:22,455
we're just converting them into strings.

85
00:05:22,455 --> 00:05:28,025
So, we're basically taking the rowdict of plurality and if it is one,

86
00:05:28,025 --> 00:05:30,845
then we basically get the zeros which should be single,

87
00:05:30,845 --> 00:05:32,630
if it is two, it would be twins,

88
00:05:32,630 --> 00:05:34,735
if it's three, it'll be triplets, et cetera.

89
00:05:34,735 --> 00:05:37,810
So, that's essentially the transformation.

90
00:05:37,810 --> 00:05:39,695
This is the transformation for is_male,

91
00:05:39,695 --> 00:05:43,090
this is the transformation of plurality and having done that,

92
00:05:43,090 --> 00:05:44,605
is there anything else we need to do?

93
00:05:44,605 --> 00:05:51,015
No. So, we can now go ahead and create our csv_fields and yield all of them.

94
00:05:51,015 --> 00:05:56,185
So, what we can do is we can say for dictionary in,

95
00:05:56,185 --> 00:06:03,165
there are two dictionaries that we have, no_ultrasound and w_ultrasound.

96
00:06:03,165 --> 00:06:05,070
In both of these,

97
00:06:05,070 --> 00:06:09,920
what we want to do is to basically get all the values and that would basically

98
00:06:09,920 --> 00:06:15,050
give us the column values,

99
00:06:15,050 --> 00:06:17,700
and once we take all of those values,

100
00:06:17,700 --> 00:06:20,120
we can basically go ahead and join them.

101
00:06:20,120 --> 00:06:23,850
So, here's where I'm doing this,

102
00:06:26,440 --> 00:06:31,270
let me just put this in here and we can walk through the code.

103
00:06:31,510 --> 00:06:34,915
So, let's ignore the key for now,

104
00:06:34,915 --> 00:06:36,715
we don't really need the key.

105
00:06:36,715 --> 00:06:41,690
So, what I'm doing is that for every CSV column,

106
00:06:41,690 --> 00:06:43,710
so I want them in this particular order,

107
00:06:43,710 --> 00:06:46,035
so for k in CSV_COLUMNS,

108
00:06:46,035 --> 00:06:51,640
I'm basically taking the actual if k is in result.

109
00:06:51,640 --> 00:06:53,740
So, this is result,

110
00:06:53,830 --> 00:06:56,575
if k is in result,

111
00:06:56,575 --> 00:06:58,640
so if this column value is present,

112
00:06:58,640 --> 00:07:02,900
remember that we ran into a lot of cases where the data value was null,

113
00:07:02,900 --> 00:07:04,875
so this verifies that.

114
00:07:04,875 --> 00:07:06,075
If it is there,

115
00:07:06,075 --> 00:07:07,790
then you're basically putting that value,

116
00:07:07,790 --> 00:07:10,970
otherwise you're putting the value none and you're basically making

117
00:07:10,970 --> 00:07:15,145
everything a string so that we can basically do a join with it.

118
00:07:15,145 --> 00:07:19,100
So, this is now our data and we can now go ahead and return the data.

119
00:07:19,100 --> 00:07:21,860
But in this case, what I'm also doing is that I'm

120
00:07:21,860 --> 00:07:25,480
maintaining that besides the normal CSV columns,

121
00:07:25,480 --> 00:07:29,255
I'm also maintaining a key and I'm forming the key

122
00:07:29,255 --> 00:07:33,350
by doing a hash of the exact value and storing it.

123
00:07:33,350 --> 00:07:35,650
That way, each row will have a unique key.

124
00:07:35,650 --> 00:07:37,020
This is totally optional,

125
00:07:37,020 --> 00:07:38,730
you don't need it yet,

126
00:07:38,730 --> 00:07:40,610
but one of the things is that when you're doing

127
00:07:40,610 --> 00:07:44,185
productionization and you want to do batch prediction,

128
00:07:44,185 --> 00:07:46,249
it turns out that you will distribute

129
00:07:46,249 --> 00:07:49,280
your predictions over multiple machines and it's super

130
00:07:49,280 --> 00:07:51,950
helpful to have a unique key associated

131
00:07:51,950 --> 00:07:54,620
with every row so that when the results come back,

132
00:07:54,620 --> 00:07:56,390
you know which result,

133
00:07:56,390 --> 00:07:59,235
which prediction corresponded to which input.

134
00:07:59,235 --> 00:08:01,125
That's what the key is there for.

135
00:08:01,125 --> 00:08:04,770
Ignore it because it's not something that you would normally need,

136
00:08:04,770 --> 00:08:06,880
but you will need it if you're going to be doing

137
00:08:06,880 --> 00:08:12,500
distributed batch prediction and I've gotten into the habit of always doing it.

138
00:08:12,500 --> 00:08:16,710
So, I just do it regardless of whether I need batch prediction or not.

139
00:08:16,710 --> 00:08:19,395
I always hash all of my columns, form a key,

140
00:08:19,395 --> 00:08:23,705
attach the key to my data so it's there if I ever need to do it.

141
00:08:23,705 --> 00:08:26,190
So, that's my first to-do.

142
00:08:26,190 --> 00:08:30,745
The second to-do is to modify the query to pull up the fields that you want.

143
00:08:30,745 --> 00:08:32,565
So, let's look at these fields,

144
00:08:32,565 --> 00:08:35,655
we needed to get weight_pounds,

145
00:08:35,655 --> 00:08:40,060
is_male, mother_age, plurality, and gestation_weeks.

146
00:08:42,120 --> 00:08:45,380
So, I have all the columns that I

147
00:08:45,380 --> 00:08:49,720
want and I'm making sure that those columns actually exist.

148
00:08:49,720 --> 00:08:51,465
If it's in test mode,

149
00:08:51,465 --> 00:08:55,220
I'm basically getting the first 100 and I'm creating a training and

150
00:08:55,220 --> 00:08:58,400
evaluation dataset and then I can go ahead and run

151
00:08:58,400 --> 00:09:02,630
this to make sure that it actually runs correctly.

152
00:09:02,630 --> 00:09:04,480
Once it runs correctly,

153
00:09:04,480 --> 00:09:10,510
I can go ahead and change the in_test_mode from locally to the hashlib.

154
00:09:10,510 --> 00:09:15,445
I'm using hashlib, I'm not importing it.

155
00:09:15,445 --> 00:09:19,775
So, let me go ahead and add import hashlib,

156
00:09:19,775 --> 00:09:22,470
and then run it again.

157
00:09:22,560 --> 00:09:30,350
This time, it seems to be working and so it's now done.

158
00:09:30,350 --> 00:09:32,725
So, my code worked,

159
00:09:32,725 --> 00:09:36,515
I can go ahead and check the file that got produced.

160
00:09:36,515 --> 00:09:38,320
What was the output file?

161
00:09:38,320 --> 00:09:44,565
My output file in_test_mode is /preproc,

162
00:09:44,565 --> 00:09:46,480
so that's where I'm writing it to.

163
00:09:46,480 --> 00:09:49,745
So, let's go ahead and check if this thing got actually written.

164
00:09:49,745 --> 00:09:54,580
So, I'll go ahead and do this, I'll do!ls preproc,

165
00:09:54,580 --> 00:09:57,540
and inside my preprocessing directory,

166
00:09:57,540 --> 00:10:02,075
there I've created my eval.csv and train.csv.

167
00:10:02,075 --> 00:10:05,040
Let's make sure that those files actually look correct.

168
00:10:05,040 --> 00:10:12,640
I can do!head preproc/train.csv*.

169
00:10:12,880 --> 00:10:18,025
So, now I basically have columns,

170
00:10:18,025 --> 00:10:19,580
so that is the weight_pounds,

171
00:10:19,580 --> 00:10:23,080
this is the is_male or not, the age,

172
00:10:23,080 --> 00:10:25,085
the plurality, the mothers,

173
00:10:25,085 --> 00:10:26,640
one of them is a mother_age,

174
00:10:26,640 --> 00:10:28,505
the other one is gestation_weeks.

175
00:10:28,505 --> 00:10:33,310
Then you have the unique hash associated with that particular row.

176
00:10:33,310 --> 00:10:35,900
As I said, this would be useful if we ever need

177
00:10:35,900 --> 00:10:38,635
to do batch predictions and batch evaluations, et cetera.

178
00:10:38,635 --> 00:10:41,700
So, we know that now our train.csv is correct,

179
00:10:41,700 --> 00:10:43,835
we've tested things locally.

180
00:10:43,835 --> 00:10:46,630
I can now go back and change the in_test_mode to be

181
00:10:46,630 --> 00:10:51,120
false and notice that when we change the test mode to be false,

182
00:10:51,120 --> 00:10:54,890
we will launch a dataflow job and the output directory will

183
00:10:54,890 --> 00:10:58,810
show up in our bucket in a folder called babyweight.

184
00:10:58,810 --> 00:11:01,135
So, now if I run this cell,

185
00:11:01,135 --> 00:11:07,480
this is going to launch off a dataflow job and I can go back to my GCP Console,

186
00:11:07,480 --> 00:11:12,765
go into Dataflow, so here's Dataflow,

187
00:11:12,765 --> 00:11:18,230
and you should see that this batch job has started for

188
00:11:18,230 --> 00:11:20,690
preprocessing and we can look at

189
00:11:20,690 --> 00:11:24,000
this thing and it's going to basically read from BigQuery,

190
00:11:24,000 --> 00:11:26,440
convert it to CSV, write it out.

191
00:11:26,440 --> 00:11:28,000
This would take maybe 15,

192
00:11:28,000 --> 00:11:30,995
20 minutes and you can watch

193
00:11:30,995 --> 00:11:35,285
the number of CPUs spinning up as they are actually executing.

194
00:11:35,285 --> 00:11:39,200
Wait for the job to be done and once the job is done,

195
00:11:39,200 --> 00:11:42,080
you should be able to go into this directory

196
00:11:42,080 --> 00:11:45,465
just like I did ls on the local preproc directory,

197
00:11:45,465 --> 00:11:48,620
you can do ls on the cloud directory to make sure

198
00:11:48,620 --> 00:11:52,080
that the eval files and your train files exist.

199
00:11:52,080 --> 00:11:55,910
At this point, you basically have the files that you

200
00:11:55,910 --> 00:12:01,565
need that contain all of the data so that you can train your model in real time.

201
00:12:01,565 --> 00:12:09,240
At this point, we now have the training and evaluation datasets created at scale.

202
00:12:09,240 --> 00:12:12,210
The process is also fully automated.

203
00:12:12,210 --> 00:12:15,680
We can simply re-run the pipeline periodically

204
00:12:15,680 --> 00:12:19,950
to create a new training dataset on fresher data.

205
00:12:19,950 --> 00:12:24,725
The next step is to train the model on the larger dataset.

206
00:12:24,725 --> 00:12:30,310
While we could train on our 10,000 sample dataset on the Notebook itself,

207
00:12:30,310 --> 00:12:32,865
when it comes to larger datasets,

208
00:12:32,865 --> 00:12:37,450
we will distribute the training off to the Cloud using ML engine.

209
00:12:37,450 --> 00:12:40,090
Lets review that next.