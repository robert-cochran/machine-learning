So, let's go ahead and do our lab where we predict babyweight using BigQuery ML. We're going to set up our lab environment like we have for within Datalab. You should be setting up Datalab like you have several times using in the click labs interface. We're going to take our project, and we're going to go ahead and paste our project again. We're going to go ahead and clear this lab to make sure that we're starting fresh. Okay, so I'm going to go ahead and run these first three cells to set up our environment. So, we're using the natality dataset. That's the US births from 1969-2008, and it's in BigQuery already. So, we can start seeing everything with what's legitimate values. So, we're going to query, then we have four in year, gestation weeks, mother's age, plurality, and weight in pounds, all above zero. So, that's our valid dataset, okay? We're going to run that, when we get back is a sample of that dataset. Now, looking over this dataset, there's a few things that we could be using for prediction, okay? So, we could do some feature engineering using CAST. Using CAST what we can do is those strings are considered as categorical features. Then, the numerical ones are considered continuous. We're also going to use hashmonth, so we can repeatedly split the data without leakage. That means that we're splitting reliably between training and validation sets without leaking one into the other one, okay? So, when we run this, what we'll get is what those features look like. We'll get weight in pounds, the sex of the child, the mother's age, the plurality, and then the gestational weeks, and then we'll get our hashmonth, the thing that allows us to split between training and tests. Now, we're going to train our BigQuery ML model, okay? So, the way that we're going to do that is that first, we're going to make a dataset, a place to store it, and we'll call it demo. We can do this through Datalab by running this command in this wrapper in bash, so bq locution US mk hyphen d demo. Okay, and now create this dataset demo, that allows us for a place to store our model. Next, we're going to create our model, and like we saw in the video, it just takes two lines to create or replace model. demo.baby weight model as is. Our options, in this case, linear regression. This is a regression problem, and our label we're trying to predict weight in pounds. Then, we have these features that we want to select, and then we're going to select from there and identify that. So, when we run this, this should take about four minutes to run and will show done when we complete. So, now we're done. Several minutes later when we save the model. So, during the model training, and after the training, you can actually see the model is training evaluation stats. So, for each run, you'll get a table with the models name, plus eval, and that's created. If we go on to the BigQuery UI, you can see that while it's training. So, what we're going to look at is none of the models train, we're going to see that information here in Datalab. So, the way I'm going to do that is with a cells. So I have BigQuery query, and then I'm going to run. This select star from ML.Training_Info, and then give it the word model and then the model name. So, when I run that, I'm going to see a few columns, okay? So, some of these are going to be obvious, but some of them are specific to BQML. So, we have training run, that's going to be zero for a newly created model. So, if we updated the model, we increment that of it. There's going to be the iteration, which is the number associated with the training run. I'm going to start from zero from the first iteration. There's loss, which is the loss we have on the training set, and then the evaluation sets loss. How much it took? In milliseconds, and then the learning rate. Like we talked about in the lecture, the learning rate is set dynamically by BigQuery ML, so you don't have to worry about setting that or determining that from various types of experimentation, okay? So, what we can do is we can plot the loss against the eval loss, and what they'll give us, it'll give us a sense of how well we're doing in terms of if we're overfitting, underfitting or not. So, we're going to plot that using matplotlib, and what we'll get is align in orange, which I'll denote the loss, and the line in green which will denote the eval loss. They match pretty well, so it doesn't seem like we're overfitting. Okay, so now we have our model. Our model is trained, we evaluated it. It's not overfitting, we can do some out-of-sample prediction. So, let's make a prediction with BQML. So, the only difference between doing the training and doing the prediction is that we change from ML train to ML predict. Super simple. So, we're going to select star from ml.PREDICT, and then we're going to provide it with those features that we used in training. Then, we're going to live it 100, we're going to take the first 100 results from our prediction. What we get back is those first 100 predictions using our model. We can see the differences between the predicted weight in pounds, and the weight in pounds. So, some of these are closer than others. Around here, we have 6.67, 6.687. Closer here, we have 3.4, the real weight is 2.5. Okay, so not precise, but it does a pretty good job, and we evaluated what that loss was earlier in training. So, the original example, we are taking into account the idea that if no ultrasound had been formed, some of the features like the sex of the child, and the plurality would not be known. Okay. So what we can do is we can augment our dataset such that we mask these features, and we train a single model to deal with both of those scenarios. We also saw that the dataset size for mothers older than 45 was pretty sparse, and less than 18 was also pretty sparse. So, we're going to discretize that mother age, put them in bins. So, low for mothers younger than 18, high for mothers over 45, and then we're going to treat each of the ages between those as bins, okay? So, when we do that, we will see is that we're going to get a dataset that has some of these edges as low or high. These are treated now as categorical variables, okay? At the same time, we're going to also suppose that we don't know whether the child is male or female to simulate that lack of an ultrasound. So, when we run this piece of BigQuery, what we'll see with that added is that we will have unknown. We've also changed the plurality here. We know if there's more than one child or a single child. So, this change from 1, 2, 3, 4 to single, multiple and just take those two values. We can bring those two datasets together with this query. So, WITH with ultrasound as this query where we know the sex of the child as well as the plurality, and then without ultrasound where we don't know those factors. Then, we're going to do a union on both of those, and then that's going to be our pre-processed table from what we're going to use in a more advanced model. So, when we run that, what we'll get is a model that displays the first few rows of that. We can't see the range of what this takes on, but we have both the mask and the unmask version. Next, we're going to create a new model that incorporates that feature engineering with the mask and the unmask, and as well as the categorical variable for mother's age. We're going to use the same thing, create or replace model demo.babyweight_model_fc. It's also going to be a linear regression and we're still predicting weight in pounds, okay? We're going to use with ultrasound, without ultrasound, and we're going to put them in a united table preprocessed, and we're going to generate our model from there. This is going to take a little longer since we have doubled our dataset. So, it's going to take about 5-7 minutes to run. When we're done, we will see the word done below the cell. So, our model is done training. So, now we can do the same thing we did with the as is model. We could go to BigQuery UI to see our training stats or we can look at them here with Datalab. So, what we'll do, we'll do the same thing, we'll plot the loss against the eval loss. So, let that run and same thing, we get our loss and our eval loss curves look near identical. So we are not overfitting using the feature engineered model as well. Now, lastly, what we're going to do is we're going to make prediction of baby weight given a number of factors. So, let's say we have a baby who's sex has been turned as male whose mother is 28. Mother is also only having one child, and the baby was born at 38 weeks of pregnancy. So, do the same thing. We'll do a select star from ml.PREDICT. In this case, we're going to have for those values true as is male, 28 as mother age, 1 is plurality, and 38 as gestation weeks. We'll run that, and what we'll get is we'll get a predicted weight in pounds. So, in this case, 5.856. So, I hope you got a good handle of how to use BigQuery ML, you saw how easy it is to do a regression task with only two lines of extra SQL. You saw how to evaluate those. You saw how to feature engineer using SQL and how to predict with that new model. So, go ahead and take this, play around with it, make it your own. I really hope that BigQuery ML becomes part of your machine learning repertoire.