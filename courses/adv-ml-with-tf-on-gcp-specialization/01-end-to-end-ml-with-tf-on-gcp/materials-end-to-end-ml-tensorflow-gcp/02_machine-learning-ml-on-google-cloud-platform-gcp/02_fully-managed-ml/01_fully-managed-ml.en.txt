So we are using TensorFlow because it can scale to big data, capture many types of feature transformations, and because it has implementations of many kinds of model architectures. What else does a machine learning framework need to provide? Hyper-parameter tuning might be nice. When you do machine learning, you often pick a number of things arbitrarily. The number of nodes, the embedding, the stride size on your convolutional layer, the number of buckets inner embedding, and as your models get more and more complex, you start wondering whether you picked the right things when you trained the model. You will have to do some search in the hyper-parameter space to see if there are better choices that you could have made. So, hyper-parameter tuning might be nice. So far, we focused on training on the top part of this diagram. But besides training, you might also want to autoscale prediction code because at some point, you want to take a train the model and you want to deploy it. At that point, the performance characteristic that you're concerned about changes. Instead of thinking about how long it takes to train on your training dataset, you start thinking about how many queries per second do you need to support from your client code. This requires autoscaling the prediction code as necessary to support the users who need those predictions. Do you want all your client code creating a DNN regressor parsing in the directory name and calling predict on it? What if your model changes? What if the hyper-parameters change? The number of inputs changes? Do you really want to expose those to your users? What if the client is not Python? The answer is to wrap your model with a Rest API. You can then invoke the API from pretty much any programming language. You can put the server on the cloud and scale to as many queries as you need when you need it. But not as obvious is this question, who does the pre-processing? Who will do the input transformations on behalf of the client code? You can't pass in the raw input variables to the trained model because a trained model expects scaled transformed inputs. You also have to worry about model changes. When you do a bag of words, for example, with a certain word mapped to the number 32, the embedding might change the next model run because your input data is larger, and now that word gets mapped to a different number, say the number 56. Similarly and scaling the minimum, the maximum, the standard deviation, these can all change. So, doing the bookkeeping associated with pre-processing and feature crosses is painful and it's a major source of error. It's also near impossible to debug, so there are probably many models out there that have a training serving skew. This difference between what it was trained on and what it's being presented at prediction time. Cloud ML Engine gives you the ability to carry out machine learning that is repeatable, scalable, and tuned. Repeatable, well you need a machine learning framework that helps you handle training serving skew. The use of TensorFlow transform, for example, simplifies the bookkeeping in several common situations. It's scalable. In training, Cloud ML will help you distribute the pre-processing and training of your model. It will also help you deploy your trained model to the Cloud. This is important because you need high-quality execution, both during training and in prediction time. Of course, you can use Cloud ML Engine to do hyper-parameter tuning. The way that we will work, is that we will start in Cloud Datalab, a Jupiter notebook environment. We'll run sequel statements to aggregate the data in BigQuery and pull the data into a Pandas DataFrame. We can then do exploration and feature selection and pre-processing using Pandas, and we will write this Pandas DataFrame out to CSV. Then, we'll start experimenting with TensorFlow. Once we have a working TensorFlow model on the smaller dataset, we can then scale it out to Google Cloud Platform. We'll do this using serverless technologies. We'll do a pre-processing in Cloud Dataflow so that the pre-processing can be scaled out to many machines. We'll create sharded CSV files on cloud storage. The TensorFlow model will remain the same, but we'll train the model on Cloud ML Engine so that we get distributed training. We'll do hyper-parameter tuning and deployment also on ML Engine. So Dataflow and ML Engine, these are both serverless technologies. So, all that we will need is Python code to submit to the services. So, this is a set of labs that you will be doing. The first lab is to explore and visualize a dataset. That's what you'll do in the next module. But before we send you off to do the lab, we will talk about the problem itself, and review some concepts before you go off to do the lab. Concepts that you will need in order to accomplish the tasks in the lab.